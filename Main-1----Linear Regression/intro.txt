Model >>>>> train >>>> training data>>>> predictions

1. Supervised
2. unsupervised
3. Semi supervised learning
4. reinforcement learning


Features>> columns


Linear Regression
    x and y ----relationship-----straight line 
    simple linear regression >straight line ===== y=mx+c
                                                m:slope
                                                c:intercept
                                                x:IV
                                ek hi independent variable
                                m and c ki optimal value find out krni h
                                Best fit line == jiska error kum hoga (least error) (residulas)
                                between actual data and predictions
                                
                                Loss functions evaluate the error for a single training 
                                example, while cost functions calculate the average error
                                across the entire training dataset or a subset of it
                                
                                cost function: optimal values of m and c batata h
                                through mean square error(MSE)= 1/n sum(y-yi)^2
                                n:total number of entries
                                MSE should be minimum
                                m and c ki vo values choose krni h jissey MSE minimum aaye

                                model evaluation technique:sabki alag h
                                regression: **R^2 (yeh batat h amount of variation)
                                                range:0 to 1
                                                if R^2 tends to 1 then model msttt
                                                jitna 0 k paas utna bekaar
                                                R^2=1-(RSS/TSS)
                                                RSS:Residual sum of sqaure  RSS=MSE=1/n sum(y-yi)^2
                                                TSS: Total Sum of square    TSS=sum(yi-y(mean of y))^2

                                            **RMSE: Root Mean sqaure error
                                            root(MSE):how close are the actual values to the predicted value
                                            (inn dono me se we preffer R^2)

                                            **Mean Absolte Error(MAE):outliers se deal kr leta h
                                            orr MSE nhi kr pata h
                                            
                                            **Adjusted R^2: kuch aise features lelena jo kisi bhi dependent 
                                            variable se koi bhi corelation follow nhi
                                            krte toh R^2 uski efficiency ko increase krdeta h
                                            Adjusted R^2=1-(1-R^2)(N-1)/N-P-1
                                            N: Total Points
                                            P: number of independent features

                                            **Huber Loss: combination of MSE and MAE
                                

    multiple linear regression?straight line ==== y=m_1*x_1+m+2*x_2+...m_n*x_n+c
                                                    y=B0+B1x1+B2x2+....e

==>various == how much acctual data differ form the average 

==> kuch ase features jo ki correlation show ni karte........
===> adjusted R^2
==> R^2 increase kr deta h
==> Adjusted R^2=1-(1-R^2)(N-1)/N-P-1
==> N: Total Points
==> P: number of independent features
